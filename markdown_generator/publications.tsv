pub_date	venue	awards	authors	title	citation	url	slug	summary	description
2011-09-22	IEEE Recent Advances in Intelligent Computational Systems		"S. Deivalakshmi, S. Shekkizhar, P. Palanisamy"	"Detection and removal of Salt and Pepper noise in images by improved median filter"	"@inproceedings{deivalakshmi2011detection,
  title={Detection and removal of salt and pepper noise in images by improved median filter},
  author={Deivalakshmi, S and Sarath, S and Palanisamy, P},
  booktitle={2011 IEEE Recent Advances in Intelligent Computational Systems},
  pages={363--368},
  year={2011},
  organization={IEEE}
}"	https://ieeexplore.ieee.org/abstract/document/6069335	improved-median-filter	A methodology based on median filters for the removal of Salt and Pepper noise by its detection followed by filtering in both binary and gray level images has been proposed in this paper.	A methodology based on median filters for the removal of Salt and Pepper noise by its detection followed by filtering in both binary and gray level images has been proposed in this paper. Linear and nonlinear filters have been proposed earlier for the removal of impulse noise; however the removal of impulse noise often brings about blurring which results in edges being distorted and poor quality. Therefore the necessity to preserve the edges and fine details during filtering is the challenge faced by researchers today. The proposed method consists of noise detection followed by the removal of detected noise by median filter using selective pixels that are not noise themselves. The noise detection is based on simple thresholding of pixels. Computer simulations were carried out to analyse the performance of the proposed method and the results obtained were compared to that of conventional median filter and center weighted median (CWM) filter.
2020-05-04	"IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"		"S. Shekkizhar, A. Ortega"	Graph Construction from Data by Non-Negative Kernel Regression	"@inproceedings{shekkizhar2020graph,
  title={Graph Construction from Data by Non-Negative Kernel Regression},
  author={Shekkizhar, Sarath and Ortega, Antonio},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={3892--3896},
  year={2020},
  organization={IEEE}
}"	https://ieeexplore.ieee.org/abstract/document/9054425	nnk-graph-icassp	"Data driven graph constructions are often used in machine learning applications. However, learning an optimal graph from data is still a challenging task. K-nearest neighbor and ϵ-neighborhood methods are among the most common graph construction methods, due to their computational simplicity, but the choice of parameters such as K and ϵ associated with these methods is often ad hoc and lacks a clear interpretation."	"Data driven graph constructions are often used in machine learning applications. However, learning an optimal graph from data is still a challenging task. K-nearest neighbor and -neighborhood methods are among the most common graph construction methods, due to their computational simplicity, but the choice of parameters such as K and associated with these methods is often ad hoc and lacks a clear interpretation. The main novelty of this paper is to formulate graph construction as the problem of finding a sparse signal approximation in kernel space, and identifying key similarities between methods in signal approximation and existing graph learning methods. We propose non-negative kernel regression (NNK), an improved approach for graph construction with interesting geometric and theoretical properties. We demonstrate experimentally the efficiency of NNK graphs, their robustness to choice of sparsity K and show that they can outperform state of the art graph methods in semi supervised learning tasks."
2019-10-21	"arXiv"		"S. Shekkizhar, A. Ortega"	"Neighborhood and Graph Constructions using Non-Negative Kernel Regression"	"@misc{shekkizhar2023neighborhood,
      title={Neighborhood and Graph Constructions using Non-Negative Kernel Regression}, 
      author={Sarath Shekkizhar and Antonio Ortega},
      year={2023},
      eprint={1910.09383},
      archivePrefix={arXiv}
}"	https://arxiv.org/abs/1910.09383	nnk-graph-arxiv	"Data driven graph constructions are often used in various applications, including several machine learning tasks, where the goal is to make predictions and discover patterns. However, learning an optimal graph from data is still a challenging task. Weighted K-nearest neighbor and ϵ-neighborhood methods are among the most common graph construction methods, due to their computational simplicity but the choice of parameters such as K and ϵ associated with these methods is often ad hoc and lacks a clear interpretation."	"Data driven graph constructions are often used in various applications, including several machine learning tasks, where the goal is to make predictions and discover patterns. However, learning an optimal graph from data is still a challenging task. Weighted K-nearest neighbor and ϵ-neighborhood methods are among the most common graph construction methods, due to their computational simplicity but the choice of parameters such as K and ϵ associated with these methods is often ad hoc and lacks a clear interpretation. We formulate graph construction as the problem of finding a sparse signal approximation in kernel space, identifying key similarities between methods in signal approximation and existing graph learning methods. We propose non-negative kernel regression~(NNK), an improved approach for graph construction with interesting geometric and theoretical properties. We show experimentally the efficiency of NNK graphs, its robustness to choice of sparsity K and better performance over state of the art graph methods in semi supervised learning tasks on real world data. "
2020-10-25	IEEE International Conference on Image Processing (ICIP)	Best student paper	"S. Shekkizhar, A. Ortega"	Efficient graph construction for image representation	"@article{shekkizhar2020efficient,
  title={Efficient graph construction for image representation},
  author={Shekkizhar, Sarath and Ortega, Antonio},
  journal={arXiv preprint arXiv:2002.06662},
  year={2020}
}"	https://arxiv.org/abs/2002.06662	nnk-image-graph	"Graphs are useful to interpret widely used image processing methods, e.g., bilateral filtering, or to develop new ones, e.g., kernel based techniques. However, simple graph constructions are often used, where edge weight and connectivity depend on a few parameters. In particular, the sparsity of the graph is determined by the choice of a window size."	"Graphs are useful to interpret widely used image processing methods, e.g., bilateral filtering, or to develop new ones, e.g., kernel based techniques. However, simple graph constructions are often used, where edge weight and connectivity depend on a few parameters. In particular, the sparsity of the graph is determined by the choice of a window size. As an alternative, we extend and adapt to images recently introduced non negative kernel regression (NNK) graph construction. In NNK graphs sparsity adapts to intrinsic data properties. Moreover, while previous work considered NNK graphs in generic settings, here we develop novel algorithms that take advantage of image properties so that the NNK approach can scale to large images. Our experiments show that sparse NNK graphs achieve improved energy compaction and denoising performance when compared to using graphs directly derived from the bilateral filter. "
2020-07-20	arXiv Preprints		"S. Shekkizhar, A. Ortega"	DeepNNK: Explaining deep models and their generalization using polytope interpolation	"@article{shekkizhar2020deepnnk,
  title={DeepNNK: Explaining deep models and their generalization using polytope interpolation},
  author={Shekkizhar, Sarath and Ortega, Antonio},
  journal={arXiv preprint arXiv:2007.10505},
  year={2020}
}"	https://arxiv.org/abs/2007.10505	deepnnk	"Modern machine learning systems based on neural networks have shown great success in learning complex data patterns while being able to make good predictions on unseen data points. However, the limited interpretability of these systems hinders further progress and application to several domains in the real world."	"Modern machine learning systems based on neural networks have shown great success in learning complex data patterns while being able to make good predictions on unseen data points. However, the limited interpretability of these systems hinders further progress and application to several domains in the real world. This predicament is exemplified by time consuming model selection and the difficulties faced in predictive explainability, especially in the presence of adversarial examples. In this paper, we take a step towards better understanding of neural networks by introducing a local polytope interpolation method. The proposed Deep Non Negative Kernel regression (NNK) interpolation framework is non parametric, theoretically simple and geometrically intuitive. We demonstrate instance based explainability for deep learning models and develop a method to identify models with good generalization properties using leave one out estimation. Finally, we draw a rationalization to adversarial and generative examples which are inevitable from an interpolation view of machine learning. "
2020-09-21	IEEE International Workshop on Multimedia Signal Processing (MMSP)		"K. Nonaka, S. Shekkizhar, A. Ortega"	Graph-based Deep Learning Analysis and Instance Selection	"@inproceedings{nonaka2020graph,
  title={Graph-based Deep Learning Analysis and Instance Selection},
  author={Nonaka, Keisuke and Shekkizhar, Sarath and Ortega, Antonio},
  booktitle={MMSP 2020-2020 IEEE International Workshop on Multimedia Signal Processing (MMSP)},
  organization={IEEE}
}"	https://confcats-event-sessions.s3.amazonaws.com/mmsp20/papers/273.pdf	graph-neural-analysis	"While deep learning is a powerful tool for manyapplications, there has been only limited research about selectionof data for training, i.e., instance selection, which enhances deeplearning scalability by saving computational resources."	"While deep learning is a powerful tool for manyapplications, there has been only limited research about selectionof data for training, i.e., instance selection, which enhances deeplearning scalability by saving computational resources. This canbe attributed in part to the difficulty of interpreting deep learningmodels. While some graph-based methods have been proposed toimprove performance and interpret behavior of deep learning,the instance selection problem has not been addressed from agraph perspective. In this paper, we analyze the behavior of deeplearning outputs by using the K-nearest neighbor (KNN) graphconstruction. We observe that when a directed KNN graph isconstructed, instead of the more conventional undirected KNN, alarge number of instances become isolated nodes, i.e., they do notbelong to the directed neighborhoods of any other nodes. Basedon this, we propose two new instance selection methods, that bothlead to fewer isolated nodes, by either directly eliminating them(minimization approach) or by connecting them more stronglyto other points (maximization). Our experiments show that ourproposed maximization method leads to better performance thanrandom selection and recent methods for instance selection."
2021-05-05	IEEE Data Science and Learning Workshop (DSLW)		"S. Shekkizhar, A. Ortega"	Revisiting local neighborhood methods in machine learning	"@inproceedings{shekkizhar2021revisit,
  title={Revisiting local neighborhood methods in machine learning},
  author={Shekkizhar, Sarath and Ortega, Antonio},
  booktitle={DSLW 2021-2021 IEEE Data Science and Learning Workshop (DSLW)},
  organization={IEEE}
}"	https://ieeexplore.ieee.org/abstract/document/9523409	nnk-classifier-dslw	"Several machine learning methods leverage the idea of locality by using $k$-nearest neighbor (KNN) techniques to design better pattern recognition models.  However, the choice of KNN parameters such as $k$ is often  made experimentally, e.g., via cross-validation, leading to local neighborhoods without a clear geometric interpretation."	"Several machine learning methods leverage the idea of locality by using $k$-nearest neighbor (KNN) techniques to design better pattern recognition models.  However, the choice of KNN parameters such as $k$ is often  made experimentally, e.g., via cross-validation, leading to local neighborhoods without a clear geometric interpretation. In this paper, we replace KNN with our recently introduced polytope neighborhood scheme - Non Negative Kernel regression (NNK). NNK formulates neighborhood selection as a sparse signal approximation problem and is adaptive to the local distribution of samples in the neighborhood of the data point of interest. We analyze the benefits of local neighborhood construction based on NNK. In particular, we study the generalization properties of local interpolation using NNK and present data dependent bounds in the non asymptotic setting. The applicability of NNK in transductive few shot learning setting and for measuring distance between two datasets is demonstrated. NNK exhibits robust, superior performance in comparison to standard locally weighted neighborhood methods. "
2021-08-15	"Asilomar Conference on Signals, Systems, and Computers"		"S. Shekkizhar, A. Ortega"	Model selection and explainability in neural networks using a polytope interpolation framework	"@article{shekkizhar2020deepnnk,
  title={DeepNNK: Explaining deep models and their generalization using polytope interpolation},
  author={Shekkizhar, Sarath and Ortega, Antonio},
  journal={arXiv preprint arXiv:2007.10505},
  year={2020}
}"	https://ieeexplore.ieee.org/abstract/document/9723382	deepnnk-asilomar	"Modern machine learning systems based on neural networks have shown great success in learning complex data patterns while being able to make good predictions on unseen data points. However, the limited understanding of these systems hinders further progress and application to several domains in the real world. "	"Modern machine learning systems based on neural networks have shown great success in learning complex data patterns while being able to make good predictions on unseen data points. However, the limited understanding of these systems hinders further progress and application to several domains in the real world. This predicament is exemplified by time-consuming model selection and the difficulties faced in predictive explainability, especially in the presence of adversarial examples. In this paper, we take a step towards better understanding of neural networks by introducing a local polytope interpolation method. The proposed Deep Non Negative Kernel regression (NNK) interpolation framework is non-parametric, theoretically simple, and geometrically intuitive. We demonstrate instance based explainability and develop a method to identify models with good generalization properties using leave one out estimation."
2021-08-31	Asia Pacific Signal and Information Processing Association (APSIPA)		"D. Bonnet, A. Ortega, J.Ruiz-Hidalgo, S.Shekkizhar"	Channel-Wise Early Stopping without a ValidationSet via NNK Polytope Interpolation	"@article{bonet2021channel,
  title={Channel-Wise Early Stopping without a Validation Set via NNK Polytope Interpolation},
  author={Bonet, David and Ortega, Antonio and Ruiz-Hidalgo, Javier and Shekkizhar, Sarath},
  journal={Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)},
  year={2021}
}"	https://arxiv.org/abs/2107.12972	cw-nnk-generalization	"Convolutional neural networks (ConvNets) comprise high-dimensional feature spaces formed by the aggregation of multiple channels, where analyzing intermediate data representations and the model's evolution can be challenging owing to the curse of dimensionality. We present channel-wise DeepNNK (CW-DeepNNK)"	"State-of-the-art neural network architectures continue to scale in size and deliver impressive generalization results, although this comes at the expense of limited interpretability. In particular, a key challenge is to determine when to stop training the model, as this has a significant impact on generalization. Convolutional neural networks (ConvNets) comprise high-dimensional feature spaces formed by the aggregation of multiple channels, where analyzing intermediate data representations and the model's evolution can be challenging owing to the curse of dimensionality. We present channel-wise DeepNNK (CW-DeepNNK), a novel channel-wise generalization estimate based on non-negative kernel regression (NNK) graphs with which we perform local polytope interpolation on low-dimensional channels. This method leads to instance-based interpretability of both the learned data representations and the relationship between channels. Motivated by our observations, we use CW-DeepNNK to propose a novel early stopping criterion that (i) does not require a validation set, (ii) is based on a task performance metric, and (iii) allows stopping to be reached at different points for each channel. Our experiments demonstrate that our proposed method has advantages as compared to the standard criterion based on validation set performance."
2022-01-21	"IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"		"D. Bonnet, A. Ortega, J.Ruiz-Hidalgo, S.Shekkizhar"	Channel redundancy and overlap in convolutional neural networks with Channel-wise NNK graphs	"@inproceedings{bonet2022channel,
  title={Channel redundancy and overlap in convolutional neural networks with Channel-wise NNK graphs},
  author={Bonet, David and Ortega, Antonio and Ruiz-Hidalgo, Javier and Shekkizhar, Sarath},
  booktitle={IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pages={In press},
  year={2022}
}"	https://ieeexplore.ieee.org/abstract/document/9746186	cw-nnk-redundancy	"Feature spaces in the deep layers of convolutional neural networks (CNNs) are often very high-dimensional and difficult to interpret. However, convolutional layers consist of multiple channels that are activated by different types of inputs, which suggests that more insights may be gained by studying the channels"	"Feature spaces in the deep layers of convolutional neural networks (CNNs) are often very high-dimensional and difficult to interpret. However, convolutional layers consist of multiple channels that are activated by different types of inputs, which suggests that more insights may be gained by studying the channels and how they relate to each other. In this paper, we first analyze theoretically channel-wise non-negative kernel (CW-NNK) regression graphs, which allow us to quantify the overlap between channels and, indirectly, the intrinsic dimension of the data representation manifold. We find that redundancy between channels is significant and varies with the layer depth and the level of regularization during training. Additionally, we observe that there is a correlation between channel overlap in the last convolutional layer and generalization performance. Our experimental results demonstrate that these techniques can lead to a better understanding of deep representations. "
2022-05-15	IEEE 30th European Signal Processing Conference (EUSIPCO)		"S. Shekkizhar, A. Ortega"	NNK-Means: Data summarization using dictionary learning with non-negative kernel regression	"@misc{shekkizhar2021nnkmeans,
    title={NNK-Means: Dictionary Learning using Non-Negative Kernel regression},
    author={Sarath Shekkizhar and Antonio Ortega},
    year={2021},
    eprint={2110.08212},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}"	https://arxiv.org/abs/2110.08212	nnk-means	"An increasing number of systems are being designed by first gathering significant amounts of data, and then optimizing the system parameters directly using the obtained data. Often this is done without analyzing the dataset structure."	"An increasing number of systems are being designed by first gathering significant amounts of data, and then optimizing the system parameters directly using the obtained data. Often this is done without analyzing the dataset structure. As task complexity, data size, and parameters all increase to millions or even billions, data summarization is becoming a major challenge. In this work, we investigate data summarization via dictionary learning, leveraging the properties of recently introduced non-negative kernel regression (NNK) graphs. Our proposed NNK-Means, unlike competing techniques, such askSVD, learns geometric dictionaries with atoms that lie in the input data space. Experiments show that summaries using NNK-Meanscan provide better discrimination compared to linear and kernel versions of kMeans and kSVD. Moreover, NNK-Means has a scalable implementation, with runtime complexity similar to that of kMeans. "
2022-05-19	arXiv Preprints		"R. Cosentino, S. Shekkizhar, M. Soltanolkotabi, S. Avestimehr, A. Ortega"	The geometry of self-supervised learning models and its impact on Transfer learning	"@misc{shekkizhar2022sslgeometry,
    title={The geometry of self-supervised learning models and its impact on Transfer learning},
    author={Romain Cosentino, Sarath Shekkizhar, Mahdi Soltanolkotabi, Salman Avestimehr, Antonio Ortega},
    year={2022},
}"	https://arxiv.org/abs/2209.08622v1	ssl-geometry	"The recent popularity of SSL has led to the development of several models that make use of diverse training strategies, architectures, and data augmentation policies with no existing unified framework to study or assess their effectiveness in transfer learning."	"Self-supervised learning~(SSL) has emerged as a desirable paradigm in computer vision due to the inability of supervised models to learn representations that can generalize in domains with limited labels. The recent popularity of SSL has led to the development of several models that make use of diverse training strategies, architectures, and data augmentation policies with no existing unified framework to study or assess their effectiveness in transfer learning. We propose a data-driven geometric strategy to analyze different SSL models using local neighborhoods in the feature space induced by each. Unlike existing approaches that consider mathematical approximations of the parameters, individual components, or optimization landscape, our work aims to explore the geometric properties of the representation manifolds learned by SSL models. Our proposed manifold graph metrics~(MGMs) provide insights into the geometric similarities and differences between available SSL models, their invariances with respect to specific augmentations, and their performances on transfer learning tasks. Our key findings are two fold: (i) contrary to popular belief, the geometry of SSL models is not tied to its training paradigm (contrastive, non-contrastive, and cluster-based); (ii) we can predict the transfer learning capability for a specific model based on the geometric properties of its semantic and augmentation manifolds."
2024-04-03	IEEE Open Journal of Signal Processing		"P. Das, S. Shekkizhar, A. Ortega"	Towards a geometric understanding of Spatio Temporal Graph Convolution Networks	"@article{das2024towards,
  title={Towards a geometric understanding of Spatiotemporal Graph Convolution Networks},
  author={Das, Pratyusha and Shekkizhar, Sarath and Ortega, Antonio},
  journal={IEEE Open Journal of Signal Processing},
  year={2024},
  publisher={IEEE}
}"	https://ieeexplore.ieee.org/iel7/8782710/9006934/10518107.pdf	stgcn-geometry	"Spatio-temporal graph convolutional networks (STGCNs) have emerged as a desirable model for many applications including skeleton-based human action recognition. Despite achieving state-of-the-art performance, our limited understanding of the representations learned by these models  hinders their application in critical and real-world settings. "	"Spatio-temporal graph convolutional networks (STGCNs) have emerged as a desirable model for many applications including skeleton-based human action recognition. Despite achieving state-of-the-art performance, our limited understanding of the representations learned by these models  hinders their application in critical and real-world settings. In this paper, we first propose a data-driven  understanding of the embedding geometry induced at different layers of the STGCN using local neighborhood graphs constructed on the feature representation of input data at each layer. To do so, we develop a window based dynamic time warping~(DTW) to compute the distance between data sequences with varying temporal lengths. Secondly, we introduce a layerwise Spatio temporal Graph Gradient-weighted Class Activation Mapping for spatio-temporal data to visualize and interpret each layer of the STGCN network. We characterize the functions learned by each layer of STGCN using the label smoothness of the representation and visualize using our GradCAM approach. We show that STGCN models learn representations that capture general human motion in their initial layers and are capable of discriminating different actions only in later layers. This provides justification for experimental observations showing that fine-tuning of later layers works well for transfer between related tasks.  We also show that noise at the input has a limited effect on label smoothness, which can help justify the robustness of STGCNs to noise. "
2022-10-31	"IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"		"C. Hurtado, S. Shekkizhar, J. Ruiz-Hidalgo, A. Ortega"	Study of Manifold Geometry using Multiscale Non-Negative Kernel Graphs	"@article{hurtado2022study,
  title={Study of Manifold Geometry using Multiscale Non-Negative Kernel Graphs},
  author={Hurtado, Carlos and Shekkizhar, Sarath and Ruiz-Hidalgo, Javier and Ortega, Antonio},
  journal={arXiv preprint arXiv:2210.17475},
  year={2022}
}"	https://arxiv.org/abs/2210.17475v1	Multiscale-nnk	"Modern machine learning systems are increasingly trained on large amounts of data embedded in high-dimensional spaces. Often this is done without analyzing the structure of the dataset. In this work, we propose a framework to study the geometric structure of the data."	"Modern machine learning systems are increasingly trained on large amounts of data embedded in high-dimensional spaces. Often this is done without analyzing the structure of the dataset. In this work, we propose a framework to study the geometric structure of the data. We make use of our recently introduced non-negative kernel (NNK) regression graphs to estimate the point density, intrinsic dimension, and the linearity of the data manifold (curvature). We further generalize the graph construction and geometric estimation to multiple scale by iteratively merging neighborhoods in the input data. Our experiments demonstrate the effectiveness of our proposed approach over other baselines in estimating the local geometry of the data manifolds on synthetic and real datasets. "
2023-04-01	Graph Signal Processing Workshop 2023		"S. Shekkizhar, A. Ortega"	A data-driven graph framework for geometric understanding of deep learning	"@misc{shekkizhar2023data,
    title={A data-driven graph framework for geometric understanding of deep learning},
    author={Sarath Shekkizhar, Antonio Ortega},
    year={2023},
}"	https://gspworkshop.org/2023/	data-driven-gspw	"Deep learning approaches have achieved unprecedented performance success in many application domains. In this work, we first present an empirical framework for studying deep learning by characterizing the geometry of the data manifold in the embedding spaces, using computationally efficient graph-based methods to learn manifold properties."	"Deep learning approaches have achieved unprecedented performance success in many application domains. One of the driving factors in their success is the use of massive models; often more parameters than the size of the training dataset. It is well understood that in this overparameterized regime, deep neural networks (DNNs) are highly expressive and have the capacity to (over)fit arbitrary training data and still exhibit good generalization, i.e., performance on unseen data. In this work, we first introduce an empirical framework for studying deep learning by characterizing the geometry of the data manifold in the embedding spaces, using computationally efficient graph-based methods to learn manifold properties. We then present a study, where we ask “what do bigger models achieve that smaller models do not?” in scenarios where both achieve zero classification error on the training dataset. Our analysis shows that there exist two distinct types of learned embedding spaces within the modern interpolative learning regimes. We observe that an increasing number of data are mapped to a local patch corresponding to the same class as the size of the model increases. We hypothesize that smaller models, which are still capable of achieving perfect classification, require a weighted averaging based on their neighborhood while larger models have local class homogeneity in the embedding space and can achieve perfect classification even with unweighted averaging."
2023-06-23	"Mining and Learning with Graphs, Knowledge Discovery and Data Mining (KDD)"		"S. Shekkizhar, N. Bulut, M. Farghal, S. Tavakkol, M. Bateni, A. Nandi"	Data Sampling using Locality Sensitive Hashing for Large Scale Graph Learning	"@inproceedings{mlg2023_2,
title={Data Sampling using Locality Sensitive Hashing for Large Scale Graph Learning},
author={Sarath Shekkizhar, Neslihan Bulut, Mohamed Farghal, Sasan Tavakkol, Mohammadhossein Bateni and Animesh Nandi},
booktitle={Proceedings of the 19th International Workshop on Mining and Learning with Graphs (MLG)},
year={2023}
}"	https://www.mlgworkshop.org/2023/papers/MLG__KDD_2023_paper_2.pdf	data-sampling-mlg	"Recent works, such as GRALE, have focused on the semi-supervised setting to learn an optimal similarity function for constructing a task-optimal graph. However, in many scenarios with billions of data points and trillions of potential edges, the run-time and computational requirements for training the similarity model make these approaches impractical. In this work, we consider data sampling as a means to overcome this issue."	"An important step in graph-based data analysis and processing is the construction of similarity graphs. Recent works, such as [7, 23 ], have focused on the semi-supervised setting to learn an optimal similarity function for constructing a task-optimal graph. However, in many scenarios with billions of data points and trillions of potential edges, the run-time and computational requirements for training the similarity model make these approaches impractical. In this work, we consider data sampling as a means to overcome this issue. Unlike typical sampling use-cases which only seek diversity, the similarity-learning for graph construction problem requires data samples that are both diverse and representative of highly similar data points. We present an efficient sampling approach by taking an adaptive partition view of locality sensitive hashing. Theoretically, we show that, though the samples obtained are correlated with sampling probabilities that do not sum to one, the training loss estimated for learning the graph similarity model using our approach is unbiased with a smaller variance compared to random sampling. Experiments on public datasets demonstrate the superior generalization of similarity models learned via our sampling. In a real large-scale industrial abuse-detection example, we observe ≈10× increase in identifying abusive items while having a lower false positive rate compared to the baseline."
2024-05-04	International Conference on Machine Learning (ICML)		"R Balestriero, R Cosentino, S Shekkizhar"	"Characterizing Large Language Model Geometry Solves Toxicity Detection and Generation"	"@article{balestriero2023characterizing,
  title={Characterizing large language model geometry solves toxicity detection and generation},
  author={Balestriero, Randall and Cosentino, Romain and Shekkizhar, Sarath},
  journal={arXiv preprint arXiv:2312.01648},
  year={2023}
}"	https://arxiv.org/abs/2312.01648	lm-geometry	"Large Language Models~(LLMs) drive current AI breakthroughs despite very little being known about their internal representations, e.g., how to extract a few informative features to solve various downstream tasks. To provide a practical and principled answer, we propose to characterize LLMs from a geometric perspective."	"Large Language Models~(LLMs) drive current AI breakthroughs despite very little being known about their internal representations, e.g., how to extract a few informative features to solve various downstream tasks. To provide a practical and principled answer, we propose to characterize LLMs from a geometric perspective. We obtain in closed form (i) the intrinsic dimension in which the Multi-Head Attention embeddings are constrained to exist and (ii) the partition and per-region affine mappings of the per-layer feedforward networks. Our results are informative, do not rely on approximations, and are actionable. First, we show that, motivated by our geometric interpretation, we can bypass Llama-2's RLHF by controlling its embedding's intrinsic dimension through informed prompt manipulation. Second, we derive 7 interpretable spline features that can be extracted from any (pre-trained) LLM layer, providing a rich abstract representation of their inputs. Those features alone (224 for Mistral-7B/Llama-2-7B and 560 for Llama-2-70B) are sufficient to help solve toxicity detection, infer the domain of the prompt, and even tackle the Jigsaw challenge, which aims at characterizing the type of toxicity of various prompts. Our results demonstrate how, even in large-scale regimes, exact theoretical results can answer practical questions in language models."
2024-07-03	arXiv Preprints		"R Cosentino, S Shekkizhar"	Reasoning in Large Language Models: A Geometric Perspective	"@misc{cosentino2024reasoninglargelanguagemodels,
      title={Reasoning in Large Language Models: A Geometric Perspective}, 
      author={Romain Cosentino and Sarath Shekkizhar},
      year={2024},
      eprint={2407.02678},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.02678}, 
}"	https://arxiv.org/abs/2407.02678	lm-reasoning-geometry	"The advancement of large language models (LLMs) for real-world applications hinges critically on enhancing their reasoning capabilities. In this work, we explore the reasoning abilities of large language models (LLMs) through their geometrical understanding. "	"The advancement of large language models (LLMs) for real-world applications hinges critically on enhancing their reasoning capabilities. In this work, we explore the reasoning abilities of large language models (LLMs) through their geometrical understanding. We establish a connection between the expressive power of LLMs and the density of their self-attention graphs. Our analysis demonstrates that the density of these graphs defines the intrinsic dimension of the inputs to the MLP blocks. We demonstrate through theoretical analysis and toy examples that a higher intrinsic dimension implies a greater expressive capacity of the LLM. We further provide empirical evidence linking this geometric framework to recent advancements in methods aimed at enhancing the reasoning capabilities of LLMs."
2025-04-22	arXiv Preprints		"S Shekkizhar, R Cosentino"	AGI Is Coming... Right After AI Learns to Play Wordle	"@misc{shekkizhar2025agicomingrightai,
      title={AGI Is Coming... Right After AI Learns to Play Wordle}, 
      author={Sarath Shekkizhar and Romain Cosentino},
      year={2025},
      eprint={2504.15434},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2504.15434}, 
}"	https://arxiv.org/abs/2504.15434	agi-wordle	"This paper investigates multimodal agents, in particular, OpenAI's Computer-User Agent (CUA), trained to control and complete tasks through a standard computer interface, similar to humans."	"This paper investigates multimodal agents, in particular, OpenAI's Computer-User Agent (CUA), trained to control and complete tasks through a standard computer interface, similar to humans. We evaluated the agent's performance on the New York Times Wordle game to elicit model behaviors and identify shortcomings. Our findings revealed a significant discrepancy in the model's ability to recognize colors correctly depending on the context. The model had a 5.36% success rate over several hundred runs across a week of Wordle. Despite the immense enthusiasm surrounding AI agents and their potential to usher in Artificial General Intelligence (AGI), our findings reinforce the fact that even simple tasks present substantial challenges for today's frontier AI models. We conclude with a discussion of the potential underlying causes, implications for future development, and research directions to improve these AI systems."
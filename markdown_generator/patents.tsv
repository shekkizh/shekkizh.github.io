title	status	application_number	filing_date	abstract	inventors
Optimizing training sets used for setting up inspection-related algorithms	Granted	US10267748	2019-04-23	Methods and systems for training an inspection-related algorithm are provided. One system includes one or more computer subsystems configured for performing an initial training of an inspection-related algorithm with a labeled set of defects thereby generating an initial version of the inspection-related algorithm and applying the initial version of the inspection-related algorithm to an unlabeled set of defects. 	"Martin Plihal, Erfan Soltanmohammadi, Saravanan Paramasivam, Sairam Ravu, Ankit Jain, Sarath Shekkizhar, Prasanti Uppaluri"
Data sampling using Locality Sensitive Hashing for large scale graph learning	Pending	US63517869	2023-08-04	"An important step in graph-based data analysis and processing is the construction of similarity graphs. Recent works have focused on the semi-supervised setting to learn an optimal similarity function for constructing a task-optimal graph. However, in many scenarios with billions of data points and trillions of potential edges, the run-time and computational requirements for training the similarity model make these approaches impractical. We present an efficient sampling approach by taking an adaptive partition view of locality sensitive hashing. Theoretically, we show that, though the samples obtained are correlated with sampling probabilities that do not sum to one, the training loss estimated for learning the graph similarity model using our approach is unbiased with a smaller variance compared to random sampling."	"Sarath Shekkizhar, Neslihan Bulut, Mohamed Farghal, Sasan Tavakkol, MohammadHossein Bateni, Animesh Nandi"
Fine-tuning machine learning models while retraining accumulated knowledge	Pending	US18496698	2023-10-27	"Techniques are described herein for a method of obtaining a neural network trained using a first dataset to perform a first task. The method further comprises iteratively updating one or more weights of the neural network during a training phase. The training phase is used to teach the neural network to perform a second task using a second dataset. The one or more weights of the neural network are updated each iteration using a first projection matrix, a gradient of the one or more weights with respect to a loss function, and a second projection matrix. The method further comprises responsive to completion of the training phase, obtaining the neural network trained to perform the first task and the second task."	"Romain Cosentino, Sarath Shekkizhar, Adam Earle, Damjan Kalajdzievski, Jack Weissenberger, Itamar Arel"
Domain aware large language model governance	Pending	US18745562	2024-06-17	"Techniques are described herein for a method of decreasing the likelihood of out-of-domain LLM responses. The method includes determining, by a block of a LLM, a representation of the text input. The method further includes determining a set of coefficients based at least on a reconstruction of the text input using a dictionary and the representation of the text input. The method further includes performing a sparsity check using the set of coefficients. The method further includes generating a response to the text input based at least on the sparsity check."	"Sarath Shekkizhar, Adam Earle"
Training a target activation sparsity in a neural network	Pending	US18802235	2024-08-13	Techniques are described herein for a method of training a target activation sparsity in a neural network. The method includes obtaining a nonlinear portion of a plurality of neurons in a neural network. The neural network is trained to perform a target task. The method further includes substituting the nonlinear portion for a dynamic nonlinear portion in the plurality of neurons in the neural network. The dynamic nonlinear portion is trained to activate or deactivate one or more neurons of the plurality of neurons. The method further includes retraining the neural network using a first loss function that minimizes a loss of the target task and second loss function that minimizes a number of active neurons.	"Damjan Kalajdzievski, Romain Cosentino, Sarath Shekkizhar, Adam Earle"
Machine learning model compression	Pending	US18905761	2024-10-03	Techniques are described herein for a method of compression in large language model. The method includes determining blocks in LLM that have redundancies that are collapsible. The method further includes performing modification to the language model without retraining of the model.	"Romain Cosentino, Damjan Kalajdzievski, Sarath Shekkizhar, Adam Earle"
Knowledge base for voice large language model applications	Pending	US63752613	2025-01-31	"A voice-based agent service provides one or more voice agents to respond to voice-based requests. For example, when a question is recieved by the voice-based agent service it can be matched to a voice-ready knowledge base. The voice-ready knowledge base is obtained offline and organized as nodes in a conversation graph. The question can be matched to a node and then subsequent nodes can be predicted by a conversation model based on previously traveresed nodes and further input in realtime."	"Sarath Shekkizhar, Romain Cosentino"
Gradient-free optimization of large language models	Pending	US63752618	2025-01-31	"Gradient-free optimization of lanugage models is performed by iteratively improving the context instruction to perform a given task. This is obtained by evaluating with reference data, criteria and making use of the reasoning as signal to improve a language model until an end condition is met."	"Romain Cosentino, Sarath Shekkizhar"
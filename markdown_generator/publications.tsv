pub_date	venue	awards	authors	title	citation	url	slug	summary	description
2011-09-22	IEEE Recent Advances in Intelligent Computational Systems		S. Deivalakshmi, S. Shekkizhar, P. Palanisamy	"Detection and removal of Salt and Pepper noise in images by improved median filter
"	"@inproceedings{deivalakshmi2011detection,
  title={Detection and removal of salt and pepper noise in images by improved median filter},
  author={Deivalakshmi, S and Sarath, S and Palanisamy, P},
  booktitle={2011 IEEE Recent Advances in Intelligent Computational Systems},
  pages={363--368},
  year={2011},
  organization={IEEE}
}"	https://ieeexplore.ieee.org/abstract/document/6069335	improved-median-filter	A methodology based on median filters for the removal of Salt and Pepper noise by its detection followed by filtering in both binary and gray level images has been proposed in this paper.	A methodology based on median filters for the removal of Salt and Pepper noise by its detection followed by filtering in both binary and gray level images has been proposed in this paper. Linear and nonlinear filters have been proposed earlier for the removal of impulse noise; however the removal of impulse noise often brings about blurring which results in edges being distorted and poor quality. Therefore the necessity to preserve the edges and fine details during filtering is the challenge faced by researchers today. The proposed method consists of noise detection followed by the removal of detected noise by median filter using selective pixels that are not noise themselves. The noise detection is based on simple thresholding of pixels. Computer simulations were carried out to analyse the performance of the proposed method and the results obtained were compared to that of conventional median filter and center weighted median (CWM) filter.
2019-04-23	US Patent Office		M. Plihal, E. Soltanmohammadi, S. Paramasivam, S. Ravu, A. Jain, S. Shekkizhar, P. Uppaluri	Optimizing training sets used for setting up inspection-related algorithms	"@misc{plihal2019optimizing,
  title={Optimizing training sets used for setting up inspection-related algorithms},
  author={Plihal, Martin and Soltanmohammadi, Erfan and Paramasivam, Saravanan and Ravu, Sairam and Jain, Ankit and Shekkizhar, Sarath and Uppaluri, Prasanti},
  year={2019},
  month=apr # ""~23"",
  publisher={Google Patents},
  note={US Patent 10,267,748}
}"	https://patents.google.com/patent/US10267748B2/en	optimizing-training	Methods and systems for training an inspection-related algorithm are provided. One system includes one or more computer subsystems configured for performing an initial training of an inspection-related algorithm with a labeled set of defects thereby generating an initial version of the inspection-related algorithm and applying the initial version of the inspection-related algorithm to an unlabeled set of defects.	Methods and systems for training an inspection-related algorithm are provided. One system includes one or more computer subsystems configured for performing an initial training of an inspection-related algorithm with a labeled set of defects thereby generating an initial version of the inspection-related algorithm and applying the initial version of the inspection-related algorithm to an unlabeled set of defects. The computer subsystem(s) are also configured for altering the labeled set of defects based on results of the applying. The computer subsystem(s) may then iteratively re-train the inspection-related algorithm and alter the labeled set of defects until one or more differences between results produced by a most recent version and a previous version of the algorithm meet one or more criteria. When the one or more differences meet the one or more criteria, the most recent version of the inspection-related algorithm is outputted as the trained algorithm.
2020-05-04	IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)		S. Shekkizhar, A. Ortega	Graph Construction from Data by Non-Negative Kernel Regression	"@inproceedings{shekkizhar2020graph,
  title={Graph Construction from Data by Non-Negative Kernel Regression},
  author={Shekkizhar, Sarath and Ortega, Antonio},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={3892--3896},
  year={2020},
  organization={IEEE}
}"	https://ieeexplore.ieee.org/abstract/document/9054425	nnk-graph-icassp	Data driven graph constructions are often used in machine learning applications. However, learning an optimal graph from data is still a challenging task. K-nearest neighbor and ϵ-neighborhood methods are among the most common graph construction methods, due to their computational simplicity, but the choice of parameters such as K and ϵ associated with these methods is often ad hoc and lacks a clear interpretation.	Data driven graph constructions are often used in machine learning applications. However, learning an optimal graph from data is still a challenging task. K-nearest neighbor and -neighborhood methods are among the most common graph construction methods, due to their computational simplicity, but the choice of parameters such as K and associated with these methods is often ad hoc and lacks a clear interpretation. The main novelty of this paper is to formulate graph construction as the problem of finding a sparse signal approximation in kernel space, and identifying key similarities between methods in signal approximation and existing graph learning methods. We propose non-negative kernel regression (NNK), an improved approach for graph construction with interesting geometric and theoretical properties. We demonstrate experimentally the efficiency of NNK graphs, their robustness to choice of sparsity K and show that they can outperform state of the art graph methods in semi supervised learning tasks.
2019-10-21	arXiv Preprints		S. Shekkizhar, A. Ortega	Graph construction from data using non negative kernel regression - Journal draft	"@article{shekkizhar2019graph,
  title={Graph construction from data using non negative kernel regression (NNK graphs)},
  author={Shekkizhar, Sarath and Ortega, Antonio},
  journal={arXiv preprint arXiv:1910.09383},
  year={2019}
}"	https://arxiv.org/abs/1910.09383	nnk-graph-arxiv	Data driven graph constructions are often used in various applications, including several machine learning tasks, where the goal is to make predictions and discover patterns. However, learning an optimal graph from data is still a challenging task. Weighted K-nearest neighbor and ϵ-neighborhood methods are among the most common graph construction methods, due to their computational simplicity but the choice of parameters such as K and ϵ associated with these methods is often ad hoc and lacks a clear interpretation.	Data driven graph constructions are often used in various applications, including several machine learning tasks, where the goal is to make predictions and discover patterns. However, learning an optimal graph from data is still a challenging task. Weighted K-nearest neighbor and ϵ-neighborhood methods are among the most common graph construction methods, due to their computational simplicity but the choice of parameters such as K and ϵ associated with these methods is often ad hoc and lacks a clear interpretation. We formulate graph construction as the problem of finding a sparse signal approximation in kernel space, identifying key similarities between methods in signal approximation and existing graph learning methods. We propose non-negative kernel regression~(NNK), an improved approach for graph construction with interesting geometric and theoretical properties. We show experimentally the efficiency of NNK graphs, its robustness to choice of sparsity K and better performance over state of the art graph methods in semi supervised learning tasks on real world data. 
2020-10-25	IEEE International Conference on Image Processing (ICIP)	Best student paper	S. Shekkizhar, A. Ortega	Efficient graph construction for image representation	"@article{shekkizhar2020efficient,
  title={Efficient graph construction for image representation},
  author={Shekkizhar, Sarath and Ortega, Antonio},
  journal={arXiv preprint arXiv:2002.06662},
  year={2020}
}"	https://arxiv.org/abs/2002.06662	nnk-image-graph	Graphs are useful to interpret widely used image processing methods, e.g., bilateral filtering, or to develop new ones, e.g., kernel based techniques. However, simple graph constructions are often used, where edge weight and connectivity depend on a few parameters. In particular, the sparsity of the graph is determined by the choice of a window size.	Graphs are useful to interpret widely used image processing methods, e.g., bilateral filtering, or to develop new ones, e.g., kernel based techniques. However, simple graph constructions are often used, where edge weight and connectivity depend on a few parameters. In particular, the sparsity of the graph is determined by the choice of a window size. As an alternative, we extend and adapt to images recently introduced non negative kernel regression (NNK) graph construction. In NNK graphs sparsity adapts to intrinsic data properties. Moreover, while previous work considered NNK graphs in generic settings, here we develop novel algorithms that take advantage of image properties so that the NNK approach can scale to large images. Our experiments show that sparse NNK graphs achieve improved energy compaction and denoising performance when compared to using graphs directly derived from the bilateral filter. 
2020-07-20	arXiv Preprints		S. Shekkizhar, A. Ortega	DeepNNK: Explaining deep models and their generalization using polytope interpolation	"@article{shekkizhar2020deepnnk,
  title={DeepNNK: Explaining deep models and their generalization using polytope interpolation},
  author={Shekkizhar, Sarath and Ortega, Antonio},
  journal={arXiv preprint arXiv:2007.10505},
  year={2020}
}"	https://arxiv.org/abs/2007.10505	deepnnk	Modern machine learning systems based on neural networks have shown great success in learning complex data patterns while being able to make good predictions on unseen data points. However, the limited interpretability of these systems hinders further progress and application to several domains in the real world.	Modern machine learning systems based on neural networks have shown great success in learning complex data patterns while being able to make good predictions on unseen data points. However, the limited interpretability of these systems hinders further progress and application to several domains in the real world. This predicament is exemplified by time consuming model selection and the difficulties faced in predictive explainability, especially in the presence of adversarial examples. In this paper, we take a step towards better understanding of neural networks by introducing a local polytope interpolation method. The proposed Deep Non Negative Kernel regression (NNK) interpolation framework is non parametric, theoretically simple and geometrically intuitive. We demonstrate instance based explainability for deep learning models and develop a method to identify models with good generalization properties using leave one out estimation. Finally, we draw a rationalization to adversarial and generative examples which are inevitable from an interpolation view of machine learning. 
2020-09-21	IEEE International Workshop on Multimedia Signal Processing (MMSP)		K. Nonaka, S. Shekkizhar, A. Ortega	Graph-based Deep Learning Analysis and Instance Selection	"@inproceedings{nonaka2020graph,
  title={Graph-based Deep Learning Analysis and Instance Selection},
  author={Nonaka, Keisuke and Shekkizhar, Sarath and Ortega, Antonio},
  booktitle={MMSP 2020-2020 IEEE International Workshop on Multimedia Signal Processing (MMSP)},
  organization={IEEE}
}"	https://confcats-event-sessions.s3.amazonaws.com/mmsp20/papers/273.pdf	graph-neural-analysis	While deep learning is a powerful tool for manyapplications, there has been only limited research about selectionof data for training, i.e., instance selection, which enhances deeplearning scalability by saving computational resources.	While deep learning is a powerful tool for manyapplications, there has been only limited research about selectionof data for training, i.e., instance selection, which enhances deeplearning scalability by saving computational resources. This canbe attributed in part to the difficulty of interpreting deep learningmodels. While some graph-based methods have been proposed toimprove performance and interpret behavior of deep learning,the instance selection problem has not been addressed from agraph perspective. In this paper, we analyze the behavior of deeplearning outputs by using the K-nearest neighbor (KNN) graphconstruction. We observe that when a directed KNN graph isconstructed, instead of the more conventional undirected KNN, alarge number of instances become isolated nodes, i.e., they do notbelong to the directed neighborhoods of any other nodes. Basedon this, we propose two new instance selection methods, that bothlead to fewer isolated nodes, by either directly eliminating them(minimization approach) or by connecting them more stronglyto other points (maximization). Our experiments show that ourproposed maximization method leads to better performance thanrandom selection and recent methods for instance selection.
2021-05-05	IEEE Data Science and Learning Workshop (DSLW)		S. Shekkizhar, A. Ortega	Revisiting local neighborhood methods in machine learning	"@inproceedings{shekkizhar2021revisit,
  title={Revisiting local neighborhood methods in machine learning},
  author={Shekkizhar, Sarath and Ortega, Antonio},
  booktitle={DSLW 2021-2021 IEEE Data Science and Learning Workshop (DSLW)},
  organization={IEEE}
}"	https://ieeexplore.ieee.org/abstract/document/9523409	nnk-classifier-dslw	Several machine learning methods leverage the idea of locality by using $k$-nearest neighbor (KNN) techniques to design better pattern recognition models.  However, the choice of KNN parameters such as $k$ is often  made experimentally, e.g., via cross-validation, leading to local neighborhoods without a clear geometric interpretation.	Several machine learning methods leverage the idea of locality by using $k$-nearest neighbor (KNN) techniques to design better pattern recognition models.  However, the choice of KNN parameters such as $k$ is often  made experimentally, e.g., via cross-validation, leading to local neighborhoods without a clear geometric interpretation. In this paper, we replace KNN with our recently introduced polytope neighborhood scheme - Non Negative Kernel regression (NNK). NNK formulates neighborhood selection as a sparse signal approximation problem and is adaptive to the local distribution of samples in the neighborhood of the data point of interest. We analyze the benefits of local neighborhood construction based on NNK. In particular, we study the generalization properties of local interpolation using NNK and present data dependent bounds in the non asymptotic setting. The applicability of NNK in transductive few shot learning setting and for measuring distance between two datasets is demonstrated. NNK exhibits robust, superior performance in comparison to standard locally weighted neighborhood methods. 
2021-08-15	"Asilomar Conference on Signals, Systems, and Computers
"		S. Shekkizhar, A. Ortega	Model selection and explainability in neural networks using a polytope interpolation framework	"@article{shekkizhar2020deepnnk,
  title={DeepNNK: Explaining deep models and their generalization using polytope interpolation},
  author={Shekkizhar, Sarath and Ortega, Antonio},
  journal={arXiv preprint arXiv:2007.10505},
  year={2020}
}"	https://ieeexplore.ieee.org/abstract/document/9723382	deepnnk-asilomar	Modern machine learning systems based on neural networks have shown great success in learning complex data patterns while being able to make good predictions on unseen data points. However, the limited understanding of these systems hinders further progress and application to several domains in the real world. 	Modern machine learning systems based on neural networks have shown great success in learning complex data patterns while being able to make good predictions on unseen data points. However, the limited understanding of these systems hinders further progress and application to several domains in the real world. This predicament is exemplified by time-consuming model selection and the difficulties faced in predictive explainability, especially in the presence of adversarial examples. In this paper, we take a step towards better understanding of neural networks by introducing a local polytope interpolation method. The proposed Deep Non Negative Kernel regression (NNK) interpolation framework is non-parametric, theoretically simple, and geometrically intuitive. We demonstrate instance based explainability and develop a method to identify models with good generalization properties using leave one out estimation.
2021-08-31	Asia Pacific Signal and Information Processing Association (APSIPA)		D. Bonnet, A. Ortega, J.Ruiz-Hidalgo, S.Shekkizhar	Channel-Wise Early Stopping without a ValidationSet via NNK Polytope Interpolation	"@article{bonet2021channel,
  title={Channel-Wise Early Stopping without a Validation Set via NNK Polytope Interpolation},
  author={Bonet, David and Ortega, Antonio and Ruiz-Hidalgo, Javier and Shekkizhar, Sarath},
  journal={Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)},
  year={2021}
}"	https://arxiv.org/abs/2107.12972	cw-nnk-generalization	Convolutional neural networks (ConvNets) comprise high-dimensional feature spaces formed by the aggregation of multiple channels, where analyzing intermediate data representations and the model's evolution can be challenging owing to the curse of dimensionality. We present channel-wise DeepNNK (CW-DeepNNK)	State-of-the-art neural network architectures continue to scale in size and deliver impressive generalization results, although this comes at the expense of limited interpretability. In particular, a key challenge is to determine when to stop training the model, as this has a significant impact on generalization. Convolutional neural networks (ConvNets) comprise high-dimensional feature spaces formed by the aggregation of multiple channels, where analyzing intermediate data representations and the model's evolution can be challenging owing to the curse of dimensionality. We present channel-wise DeepNNK (CW-DeepNNK), a novel channel-wise generalization estimate based on non-negative kernel regression (NNK) graphs with which we perform local polytope interpolation on low-dimensional channels. This method leads to instance-based interpretability of both the learned data representations and the relationship between channels. Motivated by our observations, we use CW-DeepNNK to propose a novel early stopping criterion that (i) does not require a validation set, (ii) is based on a task performance metric, and (iii) allows stopping to be reached at different points for each channel. Our experiments demonstrate that our proposed method has advantages as compared to the standard criterion based on validation set performance.
2022-01-21	IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)		D. Bonnet, A. Ortega, J.Ruiz-Hidalgo, S.Shekkizhar	Channel redundancy and overlap in convolutional neural networks with Channel-wise NNK graphs	"@inproceedings{bonet2022channel,
  title={Channel redundancy and overlap in convolutional neural networks with Channel-wise NNK graphs},
  author={Bonet, David and Ortega, Antonio and Ruiz-Hidalgo, Javier and Shekkizhar, Sarath},
  booktitle={IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pages={In press},
  year={2022}
}"	https://ieeexplore.ieee.org/abstract/document/9746186	cw-nnk-redundancy	Feature spaces in the deep layers of convolutional neural networks (CNNs) are often very high-dimensional and difficult to interpret. However, convolutional layers consist of multiple channels that are activated by different types of inputs, which suggests that more insights may be gained by studying the channels	Feature spaces in the deep layers of convolutional neural networks (CNNs) are often very high-dimensional and difficult to interpret. However, convolutional layers consist of multiple channels that are activated by different types of inputs, which suggests that more insights may be gained by studying the channels and how they relate to each other. In this paper, we first analyze theoretically channel-wise non-negative kernel (CW-NNK) regression graphs, which allow us to quantify the overlap between channels and, indirectly, the intrinsic dimension of the data representation manifold. We find that redundancy between channels is significant and varies with the layer depth and the level of regularization during training. Additionally, we observe that there is a correlation between channel overlap in the last convolutional layer and generalization performance. Our experimental results demonstrate that these techniques can lead to a better understanding of deep representations. 
2022-05-15	IEEE 30th European Signal Processing Conference (EUSIPCO)		S. Shekkizhar, A. Ortega	NNK-Means: Data summarization using dictionary learning with non-negative kernel regression	"@misc{shekkizhar2021nnkmeans,
    title={NNK-Means: Dictionary Learning using Non-Negative Kernel regression},
    author={Sarath Shekkizhar and Antonio Ortega},
    year={2021},
    eprint={2110.08212},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}"	https://arxiv.org/abs/2110.08212	nnk-means	An increasing number of systems are being designed by first gathering significant amounts of data, and then optimizing the system parameters directly using the obtained data. Often this is done without analyzing the dataset structure.	An increasing number of systems are being designed by first gathering significant amounts of data, and then optimizing the system parameters directly using the obtained data. Often this is done without analyzing the dataset structure. As task complexity, data size, and parameters all increase to millions or even billions, data summarization is becoming a major challenge. In this work, we investigate data summarization via dictionary learning, leveraging the properties of recently introduced non-negative kernel regression (NNK) graphs. Our proposed NNK-Means, unlike competing techniques, such askSVD, learns geometric dictionaries with atoms that lie in the input data space. Experiments show that summaries using NNK-Meanscan provide better discrimination compared to linear and kernel versions of kMeans and kSVD. Moreover, NNK-Means has a scalable implementation, with runtime complexity similar to that of kMeans. 
2022-05-19	In review.		R. Cosentino, S. Shekkizhar, M. Soltanolkotabi, S. Avestimehr, A. Ortega	The geometry of self-supervised learning models and its impact on Transfer learning	"@misc{shekkizhar2022sslgeometry,
    title={The geometry of self-supervised learning models and its impact on Transfer learning},
    author={Romain Cosentino, Sarath Shekkizhar, Mahdi Soltanolkotabi, Salman Avestimehr, Antonio Ortega},
    year={2022},
}"	https://arxiv.org/abs/2209.08622v1	ssl-geometry	The recent popularity of SSL has led to the development of several models that make use of diverse training strategies, architectures, and data augmentation policies with no existing unified framework to study or assess their effectiveness in transfer learning.	"Self-supervised learning~(SSL) has emerged as a desirable paradigm in computer vision due to the inability of supervised models to learn representations that can generalize in domains with limited labels. The recent popularity of SSL has led to the development of several models that make use of diverse training strategies, architectures, and data augmentation policies with no existing unified framework to study or assess their effectiveness in transfer learning.
We propose a data-driven geometric strategy to analyze different SSL models using local neighborhoods in the feature space induced by each. Unlike existing approaches that consider mathematical approximations of the parameters, individual components, or optimization landscape, our work aims to explore the geometric properties of the representation manifolds learned by SSL models.
Our proposed manifold graph metrics~(MGMs) provide insights into the geometric similarities and differences between available SSL models, their invariances with respect to specific augmentations, and their performances on transfer learning tasks. Our key findings are two fold: (i) contrary to popular belief, the geometry of SSL models is not tied to its training paradigm (contrastive, non-contrastive, and cluster-based); (ii) we can predict the transfer learning capability for a specific model based on the geometric properties of its semantic and augmentation manifolds."
2022-08-31	In review		P. Das, S. Shekkizhar, A. Ortega	Towards a geometric understanding of Spatio Temporal Graph Convolution Networks	"@misc{das2022geometry,
    title={Towards a geometric understanding of Spatio Temporal Graph Convolution Networks},
    author={Pratyusha Das, Sarath Shekkizhar, Antonio Ortega},
    year={2022},
}"		stgcn-geometry	Spatio-temporal graph convolutional networks (STGCNs) have emerged as a desirable model for many applications including skeleton-based human action recognition. Despite achieving state-of-the-art performance, our limited understanding of the representations learned by these models  hinders their application in critical and real-world settings. 	"Spatio-temporal graph convolutional networks (STGCNs) have emerged as a desirable model for many applications including skeleton-based human action recognition. Despite achieving state-of-the-art performance, our limited understanding of the representations learned by these models  hinders their application in critical and real-world settings. 
In this paper, we first propose a data-driven  understanding of the embedding geometry induced at different layers of the STGCN using local neighborhood graphs constructed on the feature representation of input data at each layer. To do so, we develop a window based dynamic time warping~(DTW) to compute the distance between data sequences with varying temporal lengths. 
Secondly, we introduce a layerwise Spatio temporal Graph Gradient-weighted Class Activation Mapping for spatio-temporal data to visualize and interpret each layer of the STGCN network.
We characterize the functions learned by each layer of STGCN using the label smoothness of the representation and visualize using our GradCAM approach. 
We show that STGCN models learn representations that capture general human motion in their initial layers and are capable of discriminating different actions only in later layers.
This provides justification for experimental observations showing that fine-tuning of later layers works well for transfer between related tasks.  We also show that noise at the input has a limited effect on label smoothness, which can help justify the robustness of STGCNs to noise. "
2022-10-31	arXiv Preprints (In review)		C. Hurtado, S. Shekkizhar, J. Ruiz-Hidalgo, A. Ortega	Study of Manifold Geometry using Multiscale Non-Negative Kernel Graphs	"@article{hurtado2022study,
  title={Study of Manifold Geometry using Multiscale Non-Negative Kernel Graphs},
  author={Hurtado, Carlos and Shekkizhar, Sarath and Ruiz-Hidalgo, Javier and Ortega, Antonio},
  journal={arXiv preprint arXiv:2210.17475},
  year={2022}
}
"	https://arxiv.org/abs/2210.17475v1	Multiscale-nnk	Modern machine learning systems are increasingly trained on large amounts of data embedded in high-dimensional spaces. Often this is done without analyzing the structure of the dataset. In this work, we propose a framework to study the geometric structure of the data.	Modern machine learning systems are increasingly trained on large amounts of data embedded in high-dimensional spaces. Often this is done without analyzing the structure of the dataset. In this work, we propose a framework to study the geometric structure of the data. We make use of our recently introduced non-negative kernel (NNK) regression graphs to estimate the point density, intrinsic dimension, and the linearity of the data manifold (curvature). We further generalize the graph construction and geometric estimation to multiple scale by iteratively merging neighborhoods in the input data. Our experiments demonstrate the effectiveness of our proposed approach over other baselines in estimating the local geometry of the data manifolds on synthetic and real datasets. 
